{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-24T17:52:29.678899Z",
     "end_time": "2023-08-24T17:52:42.711523Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data= fetch_openml('mnist_784', version=1, parser=\"auto\")  # data from https://www.openml.org/d/554\n",
    "dfData = pd.DataFrame(np.c_[data[\"data\"], data[\"target\"]], columns = data[\"feature_names\"]+[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-24T17:52:42.719522Z",
     "end_time": "2023-08-24T17:52:43.763769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making part of our data null\n",
    "fivePer = int(0.05*dfData.shape[0])\n",
    "allInds = np.arange(0,dfData.shape[0],1)\n",
    "for col in dfData:\n",
    "    if col == \"target\":\n",
    "        continue\n",
    "    # get at most 5% unique indices and set those values to null\n",
    "    indsToNull = np.unique(np.random.choice(allInds,replace=True,size=fivePer))\n",
    "    dfData[col].iloc[indsToNull] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-24T17:52:43.768283Z",
     "end_time": "2023-08-24T17:52:49.644942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null containing rows: 70000\n",
      "Number of null containing columns: 784\n",
      "Data shape: (70000, 785)\n"
     ]
    }
   ],
   "source": [
    "# High number of rows and columns means it's very likely every row and column contains null values\n",
    "print(\"Number of null containing rows:\",\n",
    "      pd.isnull(dfData.astype(float).sum(axis=1,skipna=False)).sum())\n",
    "print(\"Number of null containing columns:\",\n",
    "      pd.isnull(dfData.astype(float).sum(axis=0,skipna=False)).sum())\n",
    "print(\"Data shape:\",dfData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-24T17:52:49.650870Z",
     "end_time": "2023-08-24T17:52:57.529299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to train with null values\n",
      "Input X contains NaN.\n",
      "LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "Can't train with null values\n"
     ]
    }
   ],
   "source": [
    "stratSplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "for train_index, test_index in stratSplit.split(dfData[data[\"feature_names\"]], dfData[\"target\"]):\n",
    "    X_train = dfData[data[\"feature_names\"]].iloc[train_index].reset_index(drop=True)\n",
    "    X_test = dfData[data[\"feature_names\"]].iloc[test_index].reset_index(drop=True)\n",
    "    \n",
    "    y_train = dfData[\"target\"].iloc[train_index].reset_index(drop=True)\n",
    "    y_test = dfData[\"target\"].iloc[test_index].reset_index(drop=True)\n",
    "    \n",
    "log_reg = LogisticRegression()\n",
    "print(\"Trying to train with null values\")\n",
    "try:\n",
    "    log_reg.fit(X_train, y_train)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Can't train with null values\")\n",
    "# print(\"Accuracy with null values in data:\",log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-24T17:52:57.528300Z",
     "end_time": "2023-08-24T17:52:57.534316Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can fill in missing data using subject/domain knowledge,\n",
    "# or going back to the source to see if missing data\n",
    "# can still be retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-24T17:52:57.538300Z",
     "end_time": "2023-08-24T17:54:11.700124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (56000, 784)\n",
      "After: (36669, 784)\n",
      "Before: (56000, 784)\n",
      "After: (55995, 784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eequa\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with dropped rows with null values: 0.9180714285714285\n"
     ]
    }
   ],
   "source": [
    "# drop rows with null values\n",
    "# tolerate at most 40 null values in a row\n",
    "numColumns = X_train.shape[1]\n",
    "X_trainDroppedRows = X_train.dropna(axis=0, thresh=numColumns-40)\n",
    "print(\"Before:\", X_train.shape)\n",
    "print(\"After:\", X_trainDroppedRows.shape)\n",
    "\n",
    "tolPercMissing = 0.08\n",
    "# tolerate at most 8% null values in a row\n",
    "X_trainDroppedRows = X_train.dropna(axis=0,thresh=numColumns-numColumns*tolPercMissing)\n",
    "X_trainDroppedRows = X_trainDroppedRows.fillna(0)\n",
    "print(\"Before:\", X_train.shape)\n",
    "print(\"After:\", X_trainDroppedRows.shape)\n",
    "remainingIndeciesTrain = X_trainDroppedRows.index\n",
    "y_trainDroppedRows = y_train.iloc[remainingIndeciesTrain]\n",
    "\n",
    "X_testDroppedRows = X_test.dropna(axis=0,thresh=numColumns-numColumns*tolPercMissing)\n",
    "remainingIndeciesTest = X_testDroppedRows.index\n",
    "y_testDroppedRows = y_test.iloc[remainingIndeciesTest]\n",
    "X_testDroppedRows = X_testDroppedRows.fillna(0)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_trainDroppedRows, y_trainDroppedRows)\n",
    "print(\"Accuracy with dropped rows with null values:\", log_reg.score(X_testDroppedRows, y_testDroppedRows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-24T17:54:11.708124Z",
     "end_time": "2023-08-24T17:55:07.904197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (56000, 784)\n",
      "After: (56000, 0)\n",
      "Before: (56000, 784)\n",
      "After: (56000, 555)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eequa\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with dropped columns with null values: 0.9147142857142857\n"
     ]
    }
   ],
   "source": [
    "# drop columns with null values\n",
    "# tolerate at most 1000 null values in a column\n",
    "numRows = X_train.shape[0]\n",
    "# tolerate at most 2000 null values in a column\n",
    "X_trainDroppedColumns = X_train.dropna(axis=1, thresh=numRows-2000)\n",
    "print(\"Before:\", X_train.shape)\n",
    "print(\"After:\", X_trainDroppedColumns.shape)\n",
    "\n",
    "tolPercMissing = 0.049\n",
    "# tolerate at most 4.9% null values in a column\n",
    "# (in this case we know that at most 5% of each column is null)\n",
    "X_trainDroppedColumns = X_train.dropna(axis=1, thresh=numRows-numRows*tolPercMissing)\n",
    "print(\"Before:\", X_train.shape)\n",
    "print(\"After:\", X_trainDroppedColumns.shape)\n",
    "remainingColumns = X_trainDroppedColumns.columns\n",
    "\n",
    "X_trainDroppedColumns = X_trainDroppedColumns.fillna(0)\n",
    "X_testDroppedColumns = X_test[remainingColumns].fillna(0)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_trainDroppedColumns,y_train)\n",
    "print(\"Accuracy with dropped columns with null values:\", log_reg.score(X_testDroppedColumns, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-24T17:55:07.878642Z",
     "end_time": "2023-08-24T17:55:33.660041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with mean imputed null values: 0.9186428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eequa\\Anaconda3\\envs\\ai\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Simple Imputing\n",
    "from sklearn.impute import SimpleImputer\n",
    "mean_imputer = SimpleImputer(strategy=\"mean\")#\"mean\",\"median\",\"most_frequent\",\"constant\"\n",
    "#                        fill_value=0.5)\n",
    "\n",
    "X_train_mean_imputed = mean_imputer.fit_transform(X_train)\n",
    "X_test_mean_imputed = mean_imputer.transform(X_test)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_mean_imputed, y_train)\n",
    "print(\"Accuracy with mean imputed null values:\", log_reg.score(X_test_mean_imputed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# KNN Imputing\n",
    "from sklearn.impute import KNNImputer\n",
    "# Find N nearest samples (if features that neither is missing are close)\n",
    "# Using 1 because large sample size\n",
    "# and otherwise finding higher number of nearest neighbours would take long\n",
    "knn_imputer = KNNImputer(n_neighbors=1)  # different distance definitions (documentation)\n",
    "\n",
    "X_train_knn_imputed = knn_imputer.fit_transform(X_train)\n",
    "X_test_knn_imputed = knn_imputer.transform(X_test)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_knn_imputed, y_train)\n",
    "print(\"Accuracy with knn imputed null values:\", log_reg.score(X_test_knn_imputed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# custom imputing based on knowledge of a problem\n",
    "def getNearestPixels(row, column, maxRow, maxColumn):\n",
    "    nearestpixels = []\n",
    "    # left\n",
    "    if column>0:\n",
    "        nearestpixels.append([row, column-1])\n",
    "        # left diagonal down\n",
    "        if row<maxRow:\n",
    "            nearestpixels.append([row+1, column-1])\n",
    "        # left diagonal up\n",
    "        if row>0:\n",
    "            nearestpixels.append([row-1, column-1])\n",
    "    # right\n",
    "    if column<maxColumn:\n",
    "        nearestpixels.append([row, column+1])\n",
    "        # right diagonal down\n",
    "        if row<maxRow:\n",
    "            nearestpixels.append([row+1, column+1])\n",
    "        # right diagonal up\n",
    "        if row>0:\n",
    "            nearestpixels.append([row-1, column+1])\n",
    "    \n",
    "    # up\n",
    "    if row>0:\n",
    "        nearestpixels.append([row-1, column])\n",
    "        \n",
    "    # down\n",
    "    if row<maxRow:\n",
    "        nearestpixels.append([row+1, column])\n",
    "    return nearestpixels\n",
    "\n",
    "def imputeRow(pixels):\n",
    "    reshaped = np.reshape(pixels.values,(int(np.sqrt(pixels.size)), -1))\n",
    "    maxRow = reshaped.shape[0]\n",
    "    maxColumn = reshaped.shape[1]\n",
    "    for row in range(maxRow):\n",
    "        for column in range(maxColumn):\n",
    "            if pd.isnull(reshaped[row,column]):\n",
    "                # if a pixel is null find the indeces of the surrounding pixels\n",
    "                nearestpixels = getNearestPixels(row, column, maxRow-1, maxColumn-1)\n",
    "                sur = []\n",
    "                # get values of surrounding pixels\n",
    "                for inds in nearestpixels:\n",
    "                    sur.append(reshaped[inds[0], inds[1]])\n",
    "                # setting value of null to mean of surrounding pixels\n",
    "                reshaped[row,column] = np.nanmean(sur)\n",
    "                \n",
    "    return reshaped.reshape(1, pixels.size)[0]\n",
    "\n",
    "# Applying function to each row\n",
    "# Transforming data since each function is returning an array,\n",
    "# sklearn doesn't like the returned dataformat\n",
    "# so we put it back into standard nested list/array format\n",
    "X_train_custom_impute = [x for x in X_train.apply(lambda row: imputeRow(row), axis=1).values]\n",
    "X_test_custom_impute = [x for x in X_test.apply(lambda row: imputeRow(row), axis=1).values]\n",
    "y_train_custom_impute = y_train.values\n",
    "y_test_custom_impute = y_test.values\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_custom_impute, y_train_custom_impute)\n",
    "print(\"Accuracy with custom imputed null values:\", log_reg.score(X_test_custom_impute, y_test_custom_impute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
